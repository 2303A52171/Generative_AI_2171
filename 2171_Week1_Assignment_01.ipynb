{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYlfOKkeR8DzRn5kY8FRi2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52171/Generative_AI_2171/blob/main/2171_Week1_Assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quesion 1"
      ],
      "metadata": {
        "id": "JKSny9lr1L8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actualvalues and deep learning model predicted values are shown in Table 1. Also compare the results with the outcomes of libraries\n",
        "\n",
        "         YActual          YPred\n",
        "           20               20.5\n",
        "           30               30.3\n",
        "           40               40.2\n",
        "           50               50.6\n",
        "           60               60.7\n",
        " Tabela 1: YActual Vs. YPred"
      ],
      "metadata": {
        "id": "8KWkeeA01VCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "\n",
        "# Actual and Predicted Values\n",
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "# 1. Mean Absolute Error (MAE)\n",
        "mae = np.mean(np.abs(y_actual - y_pred))\n",
        "print(f\"Manual MAE: {mae}\")\n",
        "\n",
        "# 2. Mean Squared Error (MSE)\n",
        "mse = np.mean((y_actual - y_pred)**2)\n",
        "print(f\"Manual MSE: {mse}\")\n",
        "\n",
        "# 3. Root Mean Squared Error (RMSE)\n",
        "rmse = math.sqrt(mse)\n",
        "print(f\"Manual RMSE: {rmse}\")\n",
        "\n",
        "# Comparison with Sklearn Metrics\n",
        "sklearn_mae = mean_absolute_error(y_actual, y_pred)\n",
        "sklearn_mse = mean_squared_error(y_actual, y_pred)\n",
        "sklearn_rmse = math.sqrt(sklearn_mse)\n",
        "\n",
        "# Print comparison\n",
        "print(\"\\nComparison with Sklearn Metrics:\")\n",
        "print(f\"Sklearn MAE: {sklearn_mae}\")\n",
        "print(f\"Sklearn MSE: {sklearn_mse}\")\n",
        "print(f\"Sklearn RMSE: {sklearn_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scv97ysg6rJW",
        "outputId": "c94d128b-472c-41b4-9278-a5cbb0592b6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual MAE: 0.4600000000000016\n",
            "Manual MSE: 0.24600000000000147\n",
            "Manual RMSE: 0.49598387070549127\n",
            "\n",
            "Comparison with Sklearn Metrics:\n",
            "Sklearn MAE: 0.4600000000000016\n",
            "Sklearn MSE: 0.24600000000000147\n",
            "Sklearn RMSE: 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "s41pWdKp7Evm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.Actual values and deep learning model predicted values are shown in Table 2. Also compare the results with outcome of libraries.\n",
        "\n",
        "         YActual     YPred\n",
        "          0            0   1 1 2 0\n",
        "          0            0   1 0 2 0\n",
        "          0            1   1 2 2 1\n",
        "          0            2   1 0 2 2\n",
        "          0            2   1 2 2 2\n",
        "\n",
        " Tabela 2: YActual Vs. YPred"
      ],
      "metadata": {
        "id": "1hm38nmz7IqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Actual and predicted values from Table 2 (binary classification)\n",
        "y_actual_class = np.array([0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 2])\n",
        "y_pred_class = np.array([0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2])\n",
        "\n",
        "# 1. Accuracy\n",
        "accuracy = accuracy_score(y_actual_class, y_pred_class)\n",
        "\n",
        "# 2. Precision\n",
        "precision = precision_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# 3. Recall\n",
        "recall = recall_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# 4. F1 Score\n",
        "f1 = f1_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# Output the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Comparing with sklearn metrics\n",
        "print(\"\\nComparison with scikit-learn metrics:\")\n",
        "print(f\"Accuracy (sklearn): {accuracy_score(y_actual_class, y_pred_class)}\")\n",
        "print(f\"Precision (sklearn): {precision_score(y_actual_class, y_pred_class, average='weighted')}\")\n",
        "print(f\"Recall (sklearn): {recall_score(y_actual_class, y_pred_class, average='weighted')}\")\n",
        "print(f\"F1 Score (sklearn): {f1_score(y_actual_class, y_pred_class, average='weighted')}\")"
      ],
      "metadata": {
        "id": "KsKB6vDy8nIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f96315-365c-4773-abeb-c1054fc43c0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7058823529411765\n",
            "Precision: 0.7198879551820729\n",
            "Recall: 0.7058823529411765\n",
            "F1 Score: 0.6988795518207284\n",
            "\n",
            "Comparison with scikit-learn metrics:\n",
            "Accuracy (sklearn): 0.7058823529411765\n",
            "Precision (sklearn): 0.7198879551820729\n",
            "Recall (sklearn): 0.7058823529411765\n",
            "F1 Score (sklearn): 0.6988795518207284\n"
          ]
        }
      ]
    }
  ]
}